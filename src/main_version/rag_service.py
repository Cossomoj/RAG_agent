import os
from dotenv import load_dotenv
import string
import asyncio
import sqlite3
import json
import re
from fastapi import FastAPI, WebSocket
import websockets
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import GigaChat
from langchain_gigachat import GigaChat
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.retrievers import EnsembleRetriever
import uvicorn

DATABASE_URL = "/app/src/main_version/AI_agent.db"

load_dotenv()


# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ FastAPI
app = FastAPI()

api_key = os.getenv("GIGACHAT_API_KEY")

folder_path_1 = os.path.join(os.path.dirname(__file__), "txt_docs/docs_pack_1")
folder_path_2 = os.path.join(os.path.dirname(__file__), "txt_docs/docs_pack_2")
folder_path_3 = os.path.join(os.path.dirname(__file__), "txt_docs/docs_pack_3")

folder_path_full = os.path.join(os.path.dirname(__file__), "txt_docs/docs_pack_full")

def create_docs_from_txt(folder_path):
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²ÑĞµÑ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² .txt Ğ² ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
    file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(".txt")]

    # Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    docs = []

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
    for file_path in file_paths:
        loader = TextLoader(file_path)
        docs.extend(loader.load())

    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,  # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¾ Ñ 500 Ğ´Ğ¾ 800 Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
        chunk_overlap=200  # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¾ Ñ 100 Ğ´Ğ¾ 200 Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ñ
    )
    split_docs = text_splitter.split_documents(docs)
    return split_docs

# Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ
split_docs_1 = create_docs_from_txt(folder_path_1)
# Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ›Ğ¸Ğ´Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ
split_docs_2 = create_docs_from_txt(folder_path_2)
# Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ PO/PM
split_docs_3 = create_docs_from_txt(folder_path_3)
# ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°ĞºĞµÑ‚
split_docs_full = create_docs_from_txt(folder_path_full)

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²
model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': False}
embedding = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° 1
vector_store_1 = FAISS.from_documents(split_docs_1, embedding=embedding)
embedding_retriever_1 = vector_store_1.as_retriever(search_kwargs={"k": 10})

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° 2
vector_store_2 = FAISS.from_documents(split_docs_2, embedding=embedding)
embedding_retriever_2 = vector_store_2.as_retriever(search_kwargs={"k": 10})

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° 3
vector_store_3 = FAISS.from_documents(split_docs_3, embedding=embedding)
embedding_retriever_3 = vector_store_3.as_retriever(search_kwargs={"k": 10})

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° ÑĞ¾ Ğ²ÑĞµĞ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
vector_store_full = FAISS.from_documents(split_docs_full, embedding=embedding)
embedding_retriever_full = vector_store_full.as_retriever(search_kwargs={"k": 15})


# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GigaChat

def create_retrieval_chain_from_folder(role, specialization, question_id, embedding_retriever, prompt_template):
    
    # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°
    template = string.Template(prompt_template)
    filled_prompt = template.substitute(role=role, specialization=specialization)

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°
    prompt = ChatPromptTemplate.from_template(filled_prompt)

    llm = GigaChat(
        credentials=api_key,
        model='GigaChat-2',
        verify_ssl_certs=False,
        profanity_check=False
    )

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸
    document_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt
    )

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ retrieval_chain
    retrieval_chain = create_retrieval_chain(embedding_retriever, document_chain)

    return retrieval_chain

def get_prompt_from_db(question_id):
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚ Ğ¸Ğ· Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ID Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°"""
    conn = sqlite3.connect(DATABASE_URL)
    cursor = conn.cursor()
    
    try:
        cursor.execute("SELECT prompt_template FROM Prompts WHERE question_id = ?", (question_id,))
        result = cursor.fetchone()
        if result:
            return result[0]
        else:
            return None
    finally:
        conn.close()



def get_best_retriever_for_role_spec(role, specialization):
    """
    Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ retriever Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ¾Ğ»Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
    """
    role_lower = role.lower() if role else ""
    spec_lower = specialization.lower() if specialization else ""
    
    # ĞœĞ°Ğ¿Ğ¿Ğ¸Ğ½Ğ³ Ñ€Ğ¾Ğ»ĞµĞ¹ Ğ½Ğ° retrievers
    if 'ÑÑ‚Ğ°Ğ¶ĞµÑ€' in role_lower or 'ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚' in role_lower:
        return embedding_retriever_1  # docs_pack_1
    elif 'Ğ»Ğ¸Ğ´' in role_lower:
        return embedding_retriever_2  # docs_pack_2
    elif 'po' in role_lower or 'pm' in role_lower:
        return embedding_retriever_3  # docs_pack_3
    else:
        # Ğ•ÑĞ»Ğ¸ Ñ€Ğ¾Ğ»ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ°Ñ, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
        if spec_lower in ['Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº', 'Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº', 'python', 'java', 'web']:
            return embedding_retriever_1  # ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
        else:
            return embedding_retriever_full  # ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ²

async def generate_semantic_search_queries(question, role, specialization):
    """
    Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
    """
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ LLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
    llm = GigaChat(
        credentials=api_key,
        model='GigaChat-2',
        verify_ssl_certs=False,
        profanity_check=False
    )
    
    # ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
    query_generation_prompt = f"""
Ğ”Ğ°Ğ½ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ: "{question}"
Ğ Ğ¾Ğ»ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ: {role if role else "Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ°"}
Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: {specialization if specialization else "Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ°"}

Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞ¹ 5-6 Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ ĞºĞ°Ñ€ÑŒĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ, Ğ˜ĞŸĞ , Ğ»Ğ¸Ğ´ĞµÑ€ÑÑ‚Ğ²Ñƒ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ¹ Ğ² IT.

ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹:
1. Ğ’ĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ·Ñ‹ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
2. Ğ¤Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ…
3. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹: "Ğ˜ĞŸĞ ", "Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ", "Ğ»Ğ¸Ğ´ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸", "Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ", "Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ¸ 1-2-1", "Ñ†ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ", "ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸", "Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ", "Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°"
4. Ğ‘Ñ‹Ñ‚ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
5. ĞŸĞ¾ĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ Ğ»Ğ¸Ğ´ĞµÑ€ÑÑ‚Ğ²Ğ°
6. Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ¾Ğ»ÑŒ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ

ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¢ĞĞ§ĞĞ«Ğ¥ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°:
- "Ğ»Ğ¸Ğ´ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ˜ĞŸĞ "
- "Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ¸"
- "Ñ†ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº"
- "Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°"
- "Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ¸ 1-2-1 Ğ»Ğ¸Ğ´ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°"
- "Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚"

ĞÑ‚Ğ²ĞµÑ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸Ğ· 5-6 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸, Ğ±ĞµĞ· Ğ½ÑƒĞ¼ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°:
"""
    
    try:
        response = await llm.ainvoke(query_generation_prompt)
        alternative_queries = [q.strip() for q in response.content.split('\n') if q.strip()]
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ ÑĞ¿Ğ¸ÑĞºĞ°
        all_queries = [question] + alternative_queries
        
        print(f"Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹:")
        for i, query in enumerate(all_queries, 1):
            print(f"  {i}. {query}")
        
        return all_queries
        
    except Exception as e:
        print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: {e}")
        return [question]  # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ² ÑĞ»ÑƒÑ‡Ğ°Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸

async def enhanced_vector_search(question, role, specialization, embedding_retriever, top_k=8):
    """
    Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
    """
    # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹
    search_queries = await generate_semantic_search_queries(question, role, specialization)
    
    # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
    all_docs = []
    doc_scores = {}  # Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ° "Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²" Ğ·Ğ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
    
    for query in search_queries:
        try:
            # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
            docs = await embedding_retriever.ainvoke(query)
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ²ĞµÑĞ°Ğ¼Ğ¸ (Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ)
            weight = 1.0 / (search_queries.index(query) + 1)  # Ğ£Ğ±Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²ĞµÑ
            
            for i, doc in enumerate(docs):
                doc_id = doc.metadata.get('source', '') + str(hash(doc.page_content[:100]))
                
                # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ÑÑ‡ĞµÑ‚Ñ‡Ğ¸Ğº Ğ´Ğ»Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {
                        'doc': doc,
                        'score': 0,
                        'query_matches': []
                    }
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ» (Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ‚Ğ¾Ğ¿Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ²)
                position_weight = 1.0 / (i + 1)
                doc_scores[doc_id]['score'] += weight * position_weight
                doc_scores[doc_id]['query_matches'].append(query)
        
        except Exception as e:
            print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° '{query}': {e}")
            continue
    
    # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ ÑÑ‡ĞµÑ‚Ñƒ
    sorted_docs = sorted(doc_scores.values(), key=lambda x: x['score'], reverse=True)
    
    # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ñ‚Ğ¾Ğ¿-K Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    result_docs = [item['doc'] for item in sorted_docs[:top_k]]
    
    print(f"\nĞ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°:")
    for i, item in enumerate(sorted_docs[:top_k], 1):
        doc = item['doc']
        score = item['score']
        source = doc.metadata.get('source', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾')
        print(f"  {i}. Ğ¡Ñ‡ĞµÑ‚: {score:.3f} | Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: {source.split('/')[-1]}")
        print(f"     Ğ¡Ğ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸: {len(item['query_matches'])}")
        print(f"     Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹: {item['query_matches'][:2]}")  # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 2 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
        print(f"     Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ: {doc.page_content[:150]}...")
        print()
    
    return result_docs

async def create_enhanced_retrieval_chain(role, specialization, question_id, embedding_retriever, prompt_template):
    """
    Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ retrieval chain Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ğ°
    """
    # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°
    template = string.Template(prompt_template)
    filled_prompt = template.substitute(
        role=role,
        specialization=specialization
    )
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ LLM
    llm = GigaChat(
        credentials=api_key,
        model='GigaChat-2',
        verify_ssl_certs=False,
        profanity_check=False
    )
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ astream
    class EnhancedRetrievalChain:
        def __init__(self, llm, filled_prompt, role, specialization, embedding_retriever):
            self.llm = llm
            self.filled_prompt = filled_prompt
            self.role = role
            self.specialization = specialization
            self.embedding_retriever = embedding_retriever
        
        async def astream(self, input_data):
            """ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°"""
            question = input_data.get('input', '')
            
            print(f"ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼: {question}")
            
            # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
            relevant_docs = await enhanced_vector_search(question, self.role, self.specialization, self.embedding_retriever)
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
            context = "\n\n".join([doc.page_content for doc in relevant_docs])
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
            final_prompt = f"""{self.filled_prompt}

ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²:
{context}

Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ: {question}

ĞÑ‚Ğ²ĞµÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ•ÑĞ»Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑƒ, Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ĞµÑ‘ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ."""
            
            print(f"ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ² GigaChat Ñ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ğ¾Ğ¼...")
            
            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ LLM
            async for chunk in self.llm.astream(final_prompt):
                if chunk and chunk.content:
                    yield {"answer": chunk.content}
    
    return EnhancedRetrievalChain(llm, filled_prompt, role, specialization, embedding_retriever)

async def create_enhanced_retrieval_chain_for_suggestions(role, specialization, user_question, bot_answer, embedding_retriever, prompt_template):
    """
    Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ retrieval chain Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ 999)
    """
    # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° ÑĞ¾ Ğ²ÑĞµĞ¼Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸
    template = string.Template(prompt_template)
    filled_prompt = template.substitute(
        role=role,
        specialization=specialization,
        user_question=user_question,
        bot_answer=bot_answer
    )
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ LLM
    llm = GigaChat(
        credentials=api_key,
        model='GigaChat-2',
        verify_ssl_certs=False,
        profanity_check=False
    )
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
    async def enhanced_suggestions_process(input_data):
        search_query = input_data.get('input', '')
        
        print(f"Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼: {search_query}")
        
        # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
        relevant_docs = await enhanced_vector_search(search_query, role, specialization, embedding_retriever)
        
        # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼
        final_prompt = f"""{filled_prompt}

Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²:
{context}

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ÑÑ‚Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ¾Ğ»ÑŒÑ {role} Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ {specialization}."""
        
        print(f"ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ² GigaChat Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²...")
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ LLM
        response = await llm.ainvoke(final_prompt)
        
        return {"answer": response.content}
    
    return enhanced_suggestions_process

#mplusk1
@app.websocket("/ws_suggest")
async def websocket_suggest_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("ws_suggest: connection accepted")
    data = await websocket.receive_text()
    print(f"ws_suggest: received data: {data}")
    payload = json.loads(data)
    
    user_question = payload.get("user_question")
    bot_answer = payload.get("bot_answer")
    role = payload.get("role")
    specialization = payload.get("specialization")

    prompt_template = get_prompt_from_db(999)
    if not prompt_template:
        print("ws_suggest: ERROR - Prompt 999 not found")
        await websocket.send_text(json.dumps({"error": "Prompt not found"}))
        await websocket.close()
        return

    # Ğ’ÑĞµĞ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ RAG Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ 999)
    use_rag = True
    print(f"ws_suggest: Using RAG for question generation")
    
    if use_rag:
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ RAG Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
        embedding_retriever = get_best_retriever_for_role_spec(role, specialization)
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ retrieval chain Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ 999)
        retrieval_chain = await create_enhanced_retrieval_chain_for_suggestions(
            role=role,
            specialization=specialization,
            user_question=user_question,
            bot_answer=bot_answer,
            embedding_retriever=embedding_retriever,
            prompt_template=prompt_template
        )
        
        try:
            print("ws_suggest: Using RAG-enhanced question generation...")
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
            search_query = f"Ğ’Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ñ‚ĞµĞ¼Ğµ: {user_question}. Ğ Ğ¾Ğ»ÑŒ: {role}. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: {specialization}"
            
            # Ğ’Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
            response = await retrieval_chain({'input': search_query})
            raw_response = response.get('answer', '')
            print(f"ws_suggest: RAG response: {raw_response}")
            
            # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
            cleaned_response = re.sub(r'^\s*\d+\.\s*', '', raw_response, flags=re.MULTILINE)
            questions = [q.strip() for q in cleaned_response.split('\n') if q.strip() and len(q.strip()) > 10]
            
            # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¾ 5 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
            questions = questions[:5]
            
        except Exception as e:
            print(f"ws_suggest: ERROR in RAG call: {e}, falling back to direct LLM")
            use_rag = False
    
    if not use_rag:
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğº LLM Ğ±ĞµĞ· RAG
        template = string.Template(prompt_template)
        filled_prompt = template.substitute(
            user_question=user_question,
            bot_answer=bot_answer,
            role=role,
            specialization=specialization
        )

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
        question_generation_guidance = f"""
Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ñ€Ğ¾Ğ»ÑŒ '{role}' Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ '{specialization}',
ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ 3-5 Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ:
1. Ğ¡Ğ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°
2. Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºÑƒ Ñ€Ğ¾Ğ»Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
3. ĞŸĞ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ ÑƒĞ³Ğ»ÑƒĞ±Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµĞ¼Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ñ‹
4. ĞœĞ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
5. ĞĞµ Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‚ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ

Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°: ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸, Ğ±ĞµĞ· Ğ½ÑƒĞ¼ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.
"""
        filled_prompt += question_generation_guidance

        llm = GigaChat(
            credentials=api_key,
            model='GigaChat-2',
            verify_ssl_certs=False,
            profanity_check=False
        )
        
        try:
            print(f"ws_suggest: Using direct LLM for question generation...")
            response = await llm.ainvoke(filled_prompt)
            print(f"ws_suggest: LLM response: {response.content}")
            
            # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ½ÑƒĞ¼ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³
            cleaned_response = re.sub(r'^\s*\d+\.\s*', '', response.content, flags=re.MULTILINE)
            questions = [q.strip() for q in cleaned_response.split('\n') if q.strip()]
        except Exception as e:
            print(f"ws_suggest: ERROR in LLM call: {e}")
            await websocket.send_text(json.dumps({"error": str(e)}))
            await websocket.close()
            return

    print(f"ws_suggest: Final questions: {questions}")
    await websocket.send_text(json.dumps(questions))
    print(f"ws_suggest: Sent questions to client: {json.dumps(questions)}")
    await websocket.close()
    print("ws_suggest: connection closed")
#mplusk2
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ WebSocket ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° GigaChat."""
    await websocket.accept()
    question = await websocket.receive_text()
    role = await websocket.receive_text()
    specialization = await websocket.receive_text()
    question_id = await websocket.receive_text()
    context = await websocket.receive_text()
    print(context)
    count = await websocket.receive_text()
    question_id = int(question_id)
    count = int(count)
    print(question)
    print(role)
    print(specialization)
    print(f"ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ {count}")
    print(f"Ğ°Ğ¹Ğ´Ğ¸ {question_id}")
    prompt_template = get_prompt_from_db(question_id)
    if not prompt_template:
        prompt_template = get_prompt_from_db(777)
    
    # Ğ”Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² 777, 888 Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ RAG
    use_rag_for_special = question_id in [777, 888]
    if use_rag_for_special:
        print(f"Special prompt {question_id}: Using RAG with enhanced vector search")
        print(f"Question: {question}")
        print(f"Role: {role}")
        print(f"Specialization: {specialization}")
        print(f"Context: {context[:100] if context else 'None'}...")

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ retrieval_chain Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞµĞ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚
    retrieval_chain = None
    should_use_rag = (
        question_id not in [777, 888, 999] or  # ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ RAG
        (question_id in [777, 888] and use_rag_for_special)  # Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
    )
    
    if should_use_rag:
        # Ğ”Ğ»Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (1-24) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
        if question_id in range(1, 25):  # Ğ’Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ 1-24
            document_path = get_specific_document_for_question(question_id, specialization)
            
            if document_path:
                print(f"ğŸ¯ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° {question_id}: {document_path}")
                retrieval_chain = create_retrieval_chain_for_specific_document(
                    role=role,
                    specialization=specialization,
                    question_id=question_id,
                    document_path=document_path,
                    prompt_template=prompt_template
                )
        
        # Ğ•ÑĞ»Ğ¸ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ chain Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹
        if not retrieval_chain:
            print(f"âš ï¸ Fallback: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ question_id={question_id}")
            
            # Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ retriever Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ question_id
            embedding_retriever = embedding_retriever_full
            if question_id in [1, 2, 3, 22, 23, 24]:
                embedding_retriever = embedding_retriever_1
            elif question_id in [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:
                embedding_retriever = embedding_retriever_2
            elif question_id in [14, 15, 16, 17, 18, 19, 20]:
                embedding_retriever = embedding_retriever_3
            elif question_id in [21]:
                embedding_retriever = embedding_retriever_full
            elif question_id in [777, 888] and use_rag_for_special:
                # Ğ”Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ retriever Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ¾Ğ»Ğ¸/ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
                embedding_retriever = get_best_retriever_for_role_spec(role, specialization)
            
            retrieval_chain = await create_enhanced_retrieval_chain(
                role=role,
                specialization=specialization,
                question_id=question_id,
                embedding_retriever=embedding_retriever,
                prompt_template=prompt_template
            )

    unwanted_chars = ["*", "**"]
    
    # Ğ­Ñ‚Ğ° Ğ²ĞµÑ‚ĞºĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ 777,888 Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ RAG
    if False:  # question_id in [777, 888] and not use_rag_for_special:
        print(f"ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ {question_id} Ğ‘Ğ•Ğ— RAG...")
        
        # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
        template = string.Template(prompt_template)
        filled_prompt = template.substitute(
            role=role,
            specialization=specialization
        )
        
        # Ğ”Ğ»Ñ ID=888 Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°
        if question_id == 888:
            if context and context != "[]":
                context_info = f"\n\nĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹:\n{context}\n\n"
                full_prompt = filled_prompt + context_info + f"Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ: {question}"
            else:
                full_prompt = filled_prompt + f"\n\nĞ’Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ: {question}"
        else:
            # Ğ”Ğ»Ñ ID=777 Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ
            full_prompt = filled_prompt + f"\n\nĞ’Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ: {question}"
        
        print(f"\n--- PROMPT Ğ”Ğ›Ğ¯ LLM (ID={question_id}, Ğ±ĞµĞ· RAG) ---\n")
        print(full_prompt)
        print("\n--- ĞšĞĞĞ•Ğ¦ PROMPT ---\n")
        
        try:
            print("ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¾Ñ‚ GigaChat...")
            chunk_count = 0
            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ GigaChat Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
            async for chunk in GigaChat(
                credentials=api_key,
                verify_ssl_certs=False,
                model='GigaChat-2'
            ).astream(full_prompt):
                if chunk and chunk.content:
                    chunk_count += 1
                    answer = chunk.content.strip()
                    
                    # Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹
                    for char in unwanted_chars:
                        answer = answer.replace(char, " ")
                        
                    answer = " ".join(answer.split())  # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹
                    
                    print(f"ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ chunk #{chunk_count}: {answer[:50]}...")
                    await websocket.send_text(answer)
            
            print(f"Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½. Ğ’ÑĞµĞ³Ğ¾ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ chunks: {chunk_count}")
            if chunk_count == 0:
                print("Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: GigaChat Ğ½Ğµ Ğ²ĞµÑ€Ğ½ÑƒĞ» Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ chunk!")
                await websocket.send_text("Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ.")
                
        except Exception as e:
            print(f"ĞĞ¨Ğ˜Ğ‘ĞšĞ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ GigaChat Ğ´Ğ»Ñ ID={question_id}: {e}")
            await websocket.send_text(f"ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°: {str(e)}")
    
    elif should_use_rag and retrieval_chain is not None:
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ RAG Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
        print(f"Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ RAG Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° {question_id}")
        
        # Ğ”Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
        if question_id in [777, 888]:
            print(f"Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° {question_id}")
            enhanced_question = question
        else:
            enhanced_question = question
        
        # Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° 888 Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğº Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑƒ
        if question_id == 888 and context and context != "[]":
            enhanced_question = f"ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹: {context}\n\nĞ¢ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ: {enhanced_question}"
            print(f"ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ 888 Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼: {enhanced_question[:200]}...")
            
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾
        async for chunk in retrieval_chain.astream({'input': enhanced_question}):
            if chunk:
                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
                answer = chunk.get("answer", "").strip()

                # Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹
                for char in unwanted_chars:
                    answer = answer.replace(char, " ")
                    
                answer = " ".join(answer.split())  # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹
                    
                await websocket.send_text(answer)  # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚

    elif not should_use_rag and question_id not in [777, 888]:
        # Fallback Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· RAG (Ğ½Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ)
        print(f"Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ {question_id} Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ±ĞµĞ· RAG!")
        template = string.Template(prompt_template)
        filled_prompt = template.substitute(role=role, specialization=specialization)
        full_prompt = filled_prompt + f"\n\nĞ’Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ: {question}"
        
        try:
            async for chunk in GigaChat(
                credentials=api_key,
                verify_ssl_certs=False,
                model='GigaChat-2'
            ).astream(full_prompt):
                if chunk and chunk.content:
                    answer = chunk.content.strip()
                    for char in unwanted_chars:
                        answer = answer.replace(char, " ")
                    answer = " ".join(answer.split())
                    await websocket.send_text(answer)
        except Exception as e:
            print(f"ĞĞ¨Ğ˜Ğ‘ĞšĞ Ğ¿Ñ€Ğ¸ fallback Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° {question_id}: {e}")
            await websocket.send_text(f"ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {str(e)}")

    elif(count > 1 and count < 10):
        for chunk in GigaChat(credentials=api_key,
                              verify_ssl_certs=False,
                                model='GigaChat-Max'
                                ).stream(f"Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ°ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹ Ğ±ĞµÑĞµĞ´Ñ‹ {context}, Ğ¾Ñ‚Ğ²ĞµÑ‚ÑŒ Ğ½Ğ° ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ {question}"):
            answer = chunk.content.strip()  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚ .content

            # Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹
            for char in unwanted_chars:
                answer = answer.replace(char, " ")

            # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹
            answer = " ".join(answer.split())

            # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· WebSocket
            await websocket.send_text(answer)


    elif(count == 101):
        for chunk in GigaChat(credentials=api_key,
                              verify_ssl_certs=False,
                                model='GigaChat-2'
                                ).stream(f"Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ½Ğ°ÑˆĞµĞ¹ Ñ Ñ‚Ğ¾Ğ±Ğ¾Ğ¹ Ğ±ĞµÑĞµĞ´Ñ‹ {context}, Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼Ğ°Ğ¹ Ğ¼Ğ½Ğµ Ñ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ"):
            answer = chunk.content.strip()  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚ .content

            # Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹
            for char in unwanted_chars:
                answer = answer.replace(char, " ")

            # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹
            answer = " ".join(answer.split())

            # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· WebSocket
            await websocket.send_text(answer)
        

    elif(count == 102):
        print("zashlo")
        for chunk in GigaChat(credentials=api_key,
                              verify_ssl_certs=False,
                                model='GigaChat-2'
                                ).stream(f"ĞĞ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸ Ğ¼Ğ½Ğµ Ğ¿Ğ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ° Ğ²Ğ¾Ñ‚ Ğ¾Ğ± ÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ğµ {context}"):
            answer = chunk.content.strip()  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚ .content

            # Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹
            for char in unwanted_chars:
                answer = answer.replace(char, " ")

            # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹
            answer = " ".join(answer.split())

            # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· WebSocket
            await websocket.send_text(answer)
    else:
        # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ»ÑƒÑ‡Ğ°Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ½Ğµ Ğ¿Ğ¾Ğ¿Ğ°Ğ»Ğ¸ Ğ½Ğ¸ Ğ² Ğ¾Ğ´Ğ½Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ
        print(f"Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸!")
        print(f"question_id: {question_id}")
        print(f"count: {count}")
        print(f"should_use_rag: {should_use_rag}")
        print(f"use_rag_for_special: {use_rag_for_special if question_id in [777, 888] else 'N/A'}")
        print(f"retrieval_chain is not None: {retrieval_chain is not None}")
        
        await websocket.send_text("Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ğµ Ñ€Ğ°Ğ·.")
            
    await websocket.close()

if __name__ == "__main__":
    print("Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑĞµÑ€Ğ²ĞµÑ€ Ğ½Ğ° ws://127.0.0.1:8000/ws")
    uvicorn.run(app, host="0.0.0.0", port=8000)

class DatabaseOperations:
    def __init__(self, db_path=DATABASE_URL):
        self.db_path = db_path

    # ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸
    def get_all_users(self):
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¾Ğ¹"""
        query = """
        SELECT u.*, 
               COUNT(DISTINCT m.message) as message_count,
               MAX(m.time) as last_activity
        FROM Users u
        LEFT JOIN Message_history m ON u.user_id = m.user_id
        GROUP BY u.user_id
        """
        # Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°

    # ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹
    def get_all_prompts(self):
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²"""
        query = "SELECT * FROM Prompts ORDER BY created_at DESC"
        # Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°

    def add_prompt(self, question_text, prompt_template):
        """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°"""
        query = """
        INSERT INTO Prompts (question_text, prompt_template)
        VALUES (?, ?)
        """
        # Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°

    def update_prompt(self, question_id, question_text, prompt_template):
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°"""
        query = """
        UPDATE Prompts 
        SET question_text = ?, prompt_template = ?
        WHERE question_id = ?
        """
        # Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°

    def delete_prompt(self, question_id):
        """Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°"""
        query = "DELETE FROM Prompts WHERE question_id = ?"
        # Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°

    # Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹
    def get_user_messages(self, user_id):
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ"""
        query = """
        SELECT * FROM Message_history 
        WHERE user_id = ? 
        ORDER BY time DESC
        """
        # Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°

class RAGDocumentManager:
    def __init__(self, base_path="app/src/main_version/txt_docs"):
        self.base_path = base_path
        self.packs = {
            "pack_1": os.path.join(base_path, "docs_pack_1"),
            "pack_2": os.path.join(base_path, "docs_pack_2"),
            "pack_3": os.path.join(base_path, "docs_pack_3"),
            "pack_full": os.path.join(base_path, "docs_pack_full")
        }

    def get_all_documents(self):
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ÑĞºĞ° Ğ²ÑĞµÑ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¿Ğ°ĞºĞµÑ‚Ğ°Ğ¼"""
        documents = {}
        for pack_name, pack_path in self.packs.items():
            documents[pack_name] = [
                f for f in os.listdir(pack_path) 
                if f.endswith('.txt')
            ]
        return documents

    def add_document(self, file_content, filename, pack_name):
        """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ² ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ°ĞºĞµÑ‚"""
        if pack_name not in self.packs:
            raise ValueError(f"ĞĞµĞ²ĞµÑ€Ğ½Ğ¾Ğµ Ğ¸Ğ¼Ñ Ğ¿Ğ°ĞºĞµÑ‚Ğ°: {pack_name}")
        
        file_path = os.path.join(self.packs[pack_name], filename)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(file_content)
        
        # Ğ¢Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² pack_full
        full_path = os.path.join(self.packs["pack_full"], filename)
        with open(full_path, 'w', encoding='utf-8') as f:
            f.write(file_content)

    def delete_document(self, filename, pack_name):
        """Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°ĞºĞµÑ‚Ğ°"""
        if pack_name not in self.packs:
            raise ValueError(f"ĞĞµĞ²ĞµÑ€Ğ½Ğ¾Ğµ Ğ¸Ğ¼Ñ Ğ¿Ğ°ĞºĞµÑ‚Ğ°: {pack_name}")
        
        file_path = os.path.join(self.packs[pack_name], filename)
        if os.path.exists(file_path):
            os.remove(file_path)
            
        # Ğ¢Ğ°ĞºĞ¶Ğµ ÑƒĞ´Ğ°Ğ»ÑĞµĞ¼ Ğ¸Ğ· pack_full
        full_path = os.path.join(self.packs["pack_full"], filename)
        if os.path.exists(full_path):
            os.remove(full_path)

    def read_document(self, filename, pack_name):
        """Ğ§Ñ‚ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°"""
        if pack_name not in self.packs:
            raise ValueError(f"ĞĞµĞ²ĞµÑ€Ğ½Ğ¾Ğµ Ğ¸Ğ¼Ñ Ğ¿Ğ°ĞºĞµÑ‚Ğ°: {pack_name}")
        
        file_path = os.path.join(self.packs[pack_name], filename)
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Ğ¤Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {filename}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

    def update_document(self, file_content, filename, pack_name):
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°"""
        self.delete_document(filename, pack_name)
        self.add_document(file_content, filename, pack_name)

# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸

def get_specific_document_for_question(question_id, specialization=None):
    """
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
    """
    # ĞœĞ°Ğ¿Ğ¿Ğ¸Ğ½Ğ³ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° ÑÑƒÑ„Ñ„Ğ¸ĞºÑÑ‹ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
    spec_mapping = {
        "python": "python",
        "java": "java", 
        "web": "web",
        "Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº": "test",
        "Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ": "test",
        "Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº": "bsa",
        "ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº": "bsa",
        "Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº": "bsa"
    }
    
    spec_suffix = spec_mapping.get(specialization.lower() if specialization else "", "python")
    
    # ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ¿Ğ¿Ğ¸Ğ½Ğ³ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
    document_mapping = {
        # Ğ’Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ²/ÑÑ‚Ğ°Ğ¶ĞµÑ€Ğ¾Ğ² (docs_pack_1)
        1: "txt_docs/docs_pack_1/Ğ1.txt",  # Ğ§Ñ‚Ğ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ñ‚ PO/PM
        2: "txt_docs/docs_pack_2/_M2_.txt",  # Ğ§Ñ‚Ğ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ñ‚ Ğ»Ğ¸Ğ´Ğ° ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸ (Ğ˜ĞŸĞ )
        3: f"txt_docs/docs_pack_1/ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ°_ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹_{spec_suffix.upper() if spec_suffix == 'python' else spec_suffix}.txt",  # ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ° ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹
        
        # Ğ’Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ Ğ»Ğ¸Ğ´Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸ (docs_pack_2)
        4: f"txt_docs/docs_pack_2/A2_{spec_suffix}.txt" if spec_suffix != "python" else "txt_docs/docs_pack_2/A2_py.txt",  # ĞĞ±ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ°
        5: f"txt_docs/docs_pack_2/Ğ¡2_{spec_suffix}.txt" if spec_suffix != "python" else "txt_docs/docs_pack_2/Ğ¡2_py.txt",  # Ğ§Ñ‚Ğ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ñ‚ PO/PM (Ğ»Ğ¸Ğ´)
        6: "txt_docs/docs_pack_2/Ğ”2.txt",  # ĞŸĞ¾Ğ¸ÑĞº ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ²
        7: "txt_docs/docs_pack_2/Ğ•2.txt",  # ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹
        8: "txt_docs/docs_pack_2/Ğ2.txt",  # Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑĞ¾ ÑÑ‚Ğ°Ğ¶ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ¶ÑƒĞ½Ğ°Ğ¼Ğ¸
        9: "txt_docs/docs_pack_2/_I2_.txt",  # ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ 1-2-1
        10: "txt_docs/docs_pack_2/_K2_.txt",  # Ğ’ÑÑ‚Ñ€ĞµÑ‡Ğ¸ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸
        11: "txt_docs/docs_pack_2/_L2_.txt",  # ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸
        12: "txt_docs/docs_pack_2/_M2_.txt",  # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ˜ĞŸĞ 
        13: "txt_docs/docs_pack_2/ĞŸ2.txt",  # ĞĞ½Ğ±Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ³ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸ĞºĞ°
        
        # Ğ’Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ PO/PM (docs_pack_3)
        14: "txt_docs/docs_pack_3/Ğ¡3.txt",  # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        15: f"txt_docs/docs_pack_3/A3_{spec_suffix}.txt" if spec_suffix != "python" else "txt_docs/docs_pack_3/A3_py.txt",  # Ğ§Ñ‚Ğ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ°
        16: f"txt_docs/docs_pack_3/Ğ‘3_{spec_suffix}.txt" if spec_suffix != "python" else "txt_docs/docs_pack_3/Ğ‘3_py.txt",  # Ğ§Ñ‚Ğ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ñ‚ Ğ»Ğ¸Ğ´Ğ° ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸
        17: "txt_docs/docs_pack_3/A3_bsa.txt",  # Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ€Ğ¾Ğ»Ğ¸ PO
        18: f"txt_docs/docs_pack_3/A3_{spec_suffix}.txt" if spec_suffix != "python" else "txt_docs/docs_pack_3/A3_py.txt",  # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ°
        19: "txt_docs/docs_pack_1/Ğ1.txt",  # Ğ§Ñ‚Ğ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ñ‚ PO/PM (Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ 1)
        20: f"txt_docs/docs_pack_3/Ğ‘3_{spec_suffix}.txt" if spec_suffix != "python" else "txt_docs/docs_pack_3/Ğ‘3_bsa.txt",  # Ğ§Ñ‚Ğ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ Ğ»Ğ¸Ğ´Ğ° ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸
        
        # Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹
        21: "txt_docs/docs_pack_full/Ğ£_1.txt",  # Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ¶ĞµÑ€Ñƒ
        22: "txt_docs/docs_pack_1/Ğ¢_1.txt",  # Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ¶ĞµÑ€Ğ°
        23: "txt_docs/docs_pack_1/Ğ”1.txt",  # SDLC
        24: "txt_docs/docs_pack_1/Ğ•1.txt",  # Ğ¢Ğ°Ğ¹Ğ¼-Ğ¼ĞµĞ½ĞµĞ´Ğ¶Ğ¼ĞµĞ½Ñ‚
    }
    
    document_path = document_mapping.get(question_id)
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ»Ğ¸ Ñ„Ğ°Ğ¹Ğ»
    if document_path:
        full_path = os.path.join(os.path.dirname(__file__), document_path)
        if not os.path.exists(full_path):
            print(f"Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {full_path}, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ fallback")
            # Fallback Ğ½Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
            fallback_mapping = {
                3: "txt_docs/docs_pack_1/ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ°_ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹_Python.txt",
                4: "txt_docs/docs_pack_2/A2_py.txt",
                5: "txt_docs/docs_pack_2/Ğ¡2_py.txt",
                15: "txt_docs/docs_pack_3/A3_py.txt",
                16: "txt_docs/docs_pack_3/Ğ‘3_py.txt",
                18: "txt_docs/docs_pack_3/A3_py.txt",
                20: "txt_docs/docs_pack_3/Ğ‘3_bsa.txt"
            }
            return fallback_mapping.get(question_id, document_path)
    
    return document_path

def load_specific_document(document_path):
    """
    Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ´Ğ»Ñ Ğ½ĞµĞ³Ğ¾ retriever
    """
    if not document_path:
        print("ĞŸÑƒÑ‚ÑŒ Ğº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñƒ Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½")
        return None
        
    full_path = os.path.join(os.path.dirname(__file__), document_path)
    
    if not os.path.exists(full_path):
        print(f"Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {full_path}")
        return None
    
    try:
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
        loader = TextLoader(full_path)
        docs = loader.load()
        
        # Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100
        )
        split_docs = text_splitter.split_documents(docs)
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
        vector_store = FAISS.from_documents(split_docs, embedding=embedding)
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})  # ĞœĞµĞ½ÑŒÑˆĞµ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
        
        print(f"âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚: {document_path} ({len(split_docs)} Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²)")
        return retriever
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° {document_path}: {e}")
        return None

def create_retrieval_chain_for_specific_document(role, specialization, question_id, document_path, prompt_template):
    """
    Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ retrieval chain Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
    """
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
    document_retriever = load_specific_document(document_path)
    
    if not document_retriever:
        print(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° {question_id}, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ fallback")
        return None
    
    # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°
    template = string.Template(prompt_template)
    filled_prompt = template.substitute(role=role, specialization=specialization)

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°
    prompt = ChatPromptTemplate.from_template(filled_prompt)

    llm = GigaChat(
        credentials=api_key,
        model='GigaChat',
        verify_ssl_certs=False,
        profanity_check=False
    )

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸
    document_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt
    )

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ retrieval_chain
    retrieval_chain = create_retrieval_chain(document_retriever, document_chain)

    return retrieval_chain
